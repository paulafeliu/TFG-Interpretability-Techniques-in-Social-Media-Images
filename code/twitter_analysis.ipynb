{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Función para procesar cada CSV y retornar el DataFrame filtrado\n",
    "def process_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    new_rows = []\n",
    "    # Iteramos sobre los índices (0 a 3) para generar las filas procesadas\n",
    "    for idx in range(4):\n",
    "        material_col = f'nep_materiality_visual_{idx}'\n",
    "        biological_col = f'nep_biological_visual_{idx}'\n",
    "        landscape_col = f'landscape-type_visual_{idx}'\n",
    "        \n",
    "        temp_df = df[['platform_id', 'nature_visual', material_col, biological_col, landscape_col]].copy()\n",
    "        temp_df['index'] = idx\n",
    "        \n",
    "        temp_df = temp_df.rename(columns={\n",
    "            material_col: 'nep_materiality_visual',\n",
    "            biological_col: 'nep_biological_visual',\n",
    "            landscape_col: 'landscape-type_visual'\n",
    "        })\n",
    "        \n",
    "        temp_df['image_name'] = temp_df['platform_id'].astype(str) + '_' + temp_df['index'].astype(str)\n",
    "        new_rows.append(temp_df)\n",
    "    \n",
    "    new_df = pd.concat(new_rows, ignore_index=True)\n",
    "    new_df = new_df.dropna(subset=['nep_materiality_visual', 'nep_biological_visual'])\n",
    "    new_df = new_df.sort_values(by=['platform_id', 'index']).reset_index(drop=True)\n",
    "    \n",
    "    # Función para procesar la columna nature_visual\n",
    "    def process_nature(val):\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "        items = [item.strip() for item in str(val).split(';')]\n",
    "        for item in items:\n",
    "            if item.lower() == \"yes\":\n",
    "                return \"Yes\"\n",
    "        return \"No\"\n",
    "    \n",
    "    # Función para procesar las demás columnas\n",
    "    def process_other(val):\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "        items = [item.strip() for item in str(val).split(';')]\n",
    "        if len(items) == 1:\n",
    "            return items[0]\n",
    "        if len(set(items)) == 1:\n",
    "            return items[0]\n",
    "        return \"; \".join(items)\n",
    "    \n",
    "    # Aplicamos las funciones a cada columna correspondiente\n",
    "    new_df['nature_visual'] = new_df['nature_visual'].apply(process_nature)\n",
    "    new_df['nep_materiality_visual'] = new_df['nep_materiality_visual'].apply(process_other)\n",
    "    new_df['nep_biological_visual'] = new_df['nep_biological_visual'].apply(process_other)\n",
    "    new_df['landscape-type_visual'] = new_df['landscape-type_visual'].apply(process_other)\n",
    "    \n",
    "    # Reordenamos las columnas según el orden deseado\n",
    "    new_order = [\n",
    "        'image_name', \n",
    "        'nature_visual', \n",
    "        'nep_materiality_visual', \n",
    "        'nep_biological_visual', \n",
    "        'landscape-type_visual', \n",
    "        'platform_id', \n",
    "        'index'\n",
    "    ]\n",
    "    new_df = new_df[new_order]\n",
    "    \n",
    "    # Función que verifica si una celda contiene más de un valor único\n",
    "    def tiene_contradicciones(valor):\n",
    "        if pd.isna(valor):\n",
    "            return False\n",
    "        items = [item.strip() for item in str(valor).split(';')]\n",
    "        return len(set(items)) > 1\n",
    "    \n",
    "    cols = ['nature_visual', 'nep_materiality_visual', 'nep_biological_visual', 'landscape-type_visual']\n",
    "    df_bool = new_df[cols].applymap(tiene_contradicciones)\n",
    "    \n",
    "    # Filtramos filas donde ninguna celda tenga contradicción\n",
    "    agreed_df = new_df[~df_bool.any(axis=1)]\n",
    "    \n",
    "    return agreed_df\n",
    "\n",
    "# Rutas de los dos archivos CSV\n",
    "csv_path1 = '/fhome/pfeliu/tfg_feliu/data/39_20250401_0816.csv'\n",
    "csv_path2 = '/fhome/pfeliu/tfg_feliu/data/43.csv'\n",
    "\n",
    "# Procesamos cada CSV\n",
    "df1 = process_csv(csv_path1)\n",
    "df2 = process_csv(csv_path2)\n",
    "\n",
    "# Concatenamos los DataFrames procesados en uno solo\n",
    "agree_df = pd.concat([df1, df2], ignore_index=True)\n",
    "agree_df = agree_df[agree_df[\"landscape-type_visual\"] != \"forest_and_seminatural_areas,water_bodies\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "agree_df.to_csv('/fhome/pfeliu/tfg_feliu/data/X_labels_agreements_0104.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset = agree_df.copy()\n",
    "\n",
    "# Clase customizada para el dataset multitarea\n",
    "class MultiTaskImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None, label_maps=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Ruta al archivo CSV con las etiquetas.\n",
    "            img_dir (string): Directorio con las imágenes.\n",
    "            transform (callable, optional): Transformaciones a aplicar a la imagen.\n",
    "            label_maps (dict, optional): Diccionario para mapear etiquetas de cada tarea a índices numéricos.\n",
    "                Ejemplo:\n",
    "                {\n",
    "                    \"nature_visual\": {\"Yes\": 1, \"No\": 0},\n",
    "                    \"nep_materiality_visual\": {\"material\": 0, \"immaterial\": 1},\n",
    "                    \"nep_biological_visual\": {\"biotic\": 0, \"abiotic\": 1},\n",
    "                    \"landscape-type_visual\": {\"artificial_surfaces\": 0, \"forest_and_seminatural_areas\": 1}\n",
    "                }\n",
    "        \"\"\"\n",
    "        #self.data = pd.read_csv(csv_file)\n",
    "        self.data = pd.read_csv(csv_file).fillna(\"NaN\")\n",
    "\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.label_maps = label_maps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Selecciona la fila del CSV\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        base_img_name = row['image_name']\n",
    "        \n",
    "        # Lista de extensiones posibles\n",
    "        extensions = ['.jpg', '.png', '.jpeg']\n",
    "        img_path = None\n",
    "        # Buscar la primera imagen que exista en el directorio\n",
    "        for ext in extensions:\n",
    "            candidate = os.path.join(self.img_dir, base_img_name + ext).replace('\\\\', '/')\n",
    "            if os.path.exists(candidate):\n",
    "                img_path = candidate\n",
    "                break\n",
    "        if img_path is None:\n",
    "            raise FileNotFoundError(f\"No se encontró la imagen: {base_img_name} con ninguna de las extensiones {extensions}\")\n",
    "        \n",
    "        # Abre la imagen y conviértela a RGB\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Extrae y (opcionalmente) mapea las etiquetas para cada tarea\n",
    "        labels = {}\n",
    "        for task in ['nature_visual', 'nep_materiality_visual', 'nep_biological_visual', 'landscape-type_visual']:\n",
    "            label_value = row[task]\n",
    "            # Si se definieron mapeos, los aplica\n",
    "            if self.label_maps and task in self.label_maps:\n",
    "                label_value = self.label_maps[task].get(label_value, -1)  # -1 o cualquier valor para etiquetas no mapeadas\n",
    "            labels[task] = label_value\n",
    "        \n",
    "        # Retorna la imagen y las etiquetas\n",
    "        return image, row['image_name'], labels\n",
    "\n",
    "# Ejemplo de transformaciones (adecuadas para modelos preentrenados en ImageNet)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Medias ImageNet\n",
    "                         std=[0.229, 0.224, 0.225])   # Desviaciones ImageNet\n",
    "])\n",
    "\n",
    "# Ejemplo de mapeo de etiquetas\n",
    "label_maps = {\n",
    "    \"nature_visual\": {\"Yes\": 1, \"No\": 0},\n",
    "    \"nep_materiality_visual\": {\"material\": 0, \"immaterial\": 1},\n",
    "    \"nep_biological_visual\": {\"biotic\": 0, \"abiotic\": 1},\n",
    "    \"landscape-type_visual\": {\"artificial_surfaces\": 0, \"forest_and_seminatural_areas\": 1, \"wetlands\": 2, \"water_bodies\": 3, \"agricultural_areas\": 4, \"other\": 5, \"none\": 6, \"NaN\": 7} \n",
    "}\n",
    "\n",
    "for col in ['nature_visual', 'nep_materiality_visual', 'nep_biological_visual', 'landscape-type_visual']:\n",
    "    print(f\"Etiquetas únicas en {col}:\")\n",
    "    print(dataset[col].unique())\n",
    "    print(\"------\")\n",
    "    print(dataset[col].value_counts())\n",
    "\n",
    "# Instanciación del dataset\n",
    "dataset = MultiTaskImageDataset(\n",
    "    csv_file=''/fhome/pfeliu/tfg_feliu/data/X_labels_agreements_0104.csv',\n",
    "    img_dir= ''/fhome/pfeliu/tfg_feliu/data/twitter',\n",
    "    transform=transform,\n",
    "    label_maps=label_maps\n",
    ")\n",
    "\n",
    "# Creación de un DataLoader (opcional, para entrenamiento)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "'''# Ejemplo de iterar sobre el dataloader\n",
    "for images, img_name, labels in dataloader:\n",
    "    # 'images' es un tensor de tamaño [batch_size, 3, 224, 224]\n",
    "    # 'labels' es un diccionario con las 4 tareas, por ejemplo:\n",
    "    #   labels['nature_visual'] -> tensor de etiquetas para la tarea nature\n",
    "    print(images.shape)\n",
    "    print(labels)\n",
    "    break'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Total de imágenes:\", len(dataset))\n",
    "print(\"Total de batches:\", len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Función para desnormalizar y mostrar la imagen con título opcional\n",
    "def imshow_tensor(image_tensor, title=None):\n",
    "    # image_tensor tiene forma [C, H, W]\n",
    "    image = image_tensor.numpy().transpose((1, 2, 0))\n",
    "    # Desnormalizamos con las medias y desviaciones utilizadas\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    plt.imshow(image)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "for idx in range(3):\n",
    "\n",
    "    # Ejemplo: mostramos la primera imagen del dataset con su nombre y etiquetas\n",
    "    sample_image, sample_image_name, sample_labels = dataset[idx]\n",
    "\n",
    "    print(\"Nombre de la imagen:\", sample_image_name)\n",
    "    print(\"Etiquetas:\")\n",
    "    for key, value in sample_labels.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "    # Mostramos la imagen y ponemos el nombre de la imagen como título\n",
    "    imshow_tensor(sample_image, title=sample_image_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Usamos ResNet50 preentrenado en ImageNet y retiramos la capa fc final\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet.fc = nn.Identity()  # Retiramos la capa fc final\n",
    "\n",
    "# Extraemos todas las capas excepto la última\n",
    "#backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "backbone = resnet\n",
    "\n",
    "# Definimos un wrapper para aplanar la salida (la salida de ResNet50 es [batch, 2048, 1, 1])\n",
    "class ResNet50Backbone(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super(ResNet50Backbone, self).__init__()\n",
    "        self.backbone = backbone\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)  # [batch, 2048, 1, 1]\n",
    "        x = torch.flatten(x, 1)  # [batch, 2048]\n",
    "        return x\n",
    "\n",
    "backbone_model = ResNet50Backbone(backbone)\n",
    "\n",
    "# Modelo MultiTarea con 4 cabezas de salida\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, backbone, feature_dim=2048):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        # Cabeza para nature_visual (2 clases)\n",
    "        self.fc_nature = nn.Linear(feature_dim, 2)\n",
    "        # Cabeza para nep_materiality_visual (2 clases)\n",
    "        self.fc_materiality = nn.Linear(feature_dim, 2)\n",
    "        # Cabeza para nep_biological_visual (2 clases)\n",
    "        self.fc_biological = nn.Linear(feature_dim, 2)\n",
    "        # Cabeza para landscape-type_visual (7 clases)\n",
    "        self.fc_landscape = nn.Linear(feature_dim, 8)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        out_nature = self.fc_nature(features)\n",
    "        out_materiality = self.fc_materiality(features)\n",
    "        out_biological = self.fc_biological(features)\n",
    "        out_landscape = self.fc_landscape(features)\n",
    "        return out_nature, out_materiality, out_biological, out_landscape\n",
    "\n",
    "model = MultiTaskModel(backbone_model)\n",
    "\n",
    "# Mover modelo al dispositivo (GPU si está disponible)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Dataset train y test\n",
    "# ================================\n",
    "\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.9 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Crear DataLoaders para cada split\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ================================\n",
    "# Entrenamiento\n",
    "# ================================\n",
    "# Definimos las funciones de pérdida para cada tarea (usando CrossEntropyLoss para clasificación)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 5  # Ajusta el número de épocas según tu necesidad\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, image_names, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        labels_nature = labels['nature_visual'].to(device)\n",
    "        labels_materiality = labels['nep_materiality_visual'].to(device)\n",
    "        labels_biological = labels['nep_biological_visual'].to(device)\n",
    "        labels_landscape = labels['landscape-type_visual'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out_nature, out_materiality, out_biological, out_landscape = model(images)\n",
    "        \n",
    "        loss_nature = criterion(out_nature, labels_nature)\n",
    "        loss_materiality = criterion(out_materiality, labels_materiality)\n",
    "        loss_biological = criterion(out_biological, labels_biological)\n",
    "        loss_landscape = criterion(out_landscape, labels_landscape)\n",
    "        \n",
    "        loss = loss_nature + loss_materiality + loss_biological + loss_landscape\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# ================================\n",
    "# Ejemplo de Predicción\n",
    "# ================================\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct_nature = 0\n",
    "correct_materiality = 0\n",
    "correct_biological = 0\n",
    "correct_landscape = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "        for images, image_names, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels_nature = labels['nature_visual'].to(device)\n",
    "            labels_materiality = labels['nep_materiality_visual'].to(device)\n",
    "            labels_biological = labels['nep_biological_visual'].to(device)\n",
    "            labels_landscape = labels['landscape-type_visual'].to(device)\n",
    "            \n",
    "            out_nature, out_materiality, out_biological, out_landscape = model(images)\n",
    "            \n",
    "            loss_nature = criterion(out_nature, labels_nature)\n",
    "            loss_materiality = criterion(out_materiality, labels_materiality)\n",
    "            loss_biological = criterion(out_biological, labels_biological)\n",
    "            loss_landscape = criterion(out_landscape, labels_landscape)\n",
    "            \n",
    "            loss = loss_nature + loss_materiality + loss_biological + loss_landscape\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Para calcular exactitud, obtenemos el índice con mayor puntuación en cada tarea\n",
    "            preds_nature = out_nature.argmax(dim=1)\n",
    "            preds_materiality = out_materiality.argmax(dim=1)\n",
    "            preds_biological = out_biological.argmax(dim=1)\n",
    "            preds_landscape = out_landscape.argmax(dim=1)\n",
    "            \n",
    "            correct_nature += (preds_nature == labels_nature).sum().item()\n",
    "            correct_materiality += (preds_materiality == labels_materiality).sum().item()\n",
    "            correct_biological += (preds_biological == labels_biological).sum().item()\n",
    "            correct_landscape += (preds_landscape == labels_landscape).sum().item()\n",
    "            total += images.size(0)\n",
    "    \n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "print(f\"Epoch {epoch+1}/{num_epochs}, Test Loss: {avg_test_loss:.4f}\")\n",
    "print(\"Accuracy por tarea:\")\n",
    "print(f\"  Nature: {correct_nature/total*100:.2f}%\")\n",
    "print(f\"  Materiality: {correct_materiality/total*100:.2f}%\")\n",
    "print(f\"  Biological: {correct_biological/total*100:.2f}%\")\n",
    "print(f\"  Landscape: {correct_landscape/total*100:.2f}%\")\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "'''# Poniendo el modelo en modo evaluación\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    images, image_names, labels = next(iter(dataloader))\n",
    "    images = images.to(device)\n",
    "    outputs = model(images)\n",
    "    # outputs es una tupla de 4 salidas\n",
    "    print(\"Predicciones de la primera batch:\")\n",
    "    print(\"Nature:\", outputs[0].argmax(dim=1))\n",
    "    print(\"Materiality:\", outputs[1].argmax(dim=1))\n",
    "    print(\"Biological:\", outputs[2].argmax(dim=1))\n",
    "    print(\"Landscape:\", outputs[3].argmax(dim=1))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to desnormalize and display an image with a title\n",
    "def imshow_tensor(image_tensor, title=None):\n",
    "    # Convert the tensor (C, H, W) to a NumPy array (H, W, C)\n",
    "    image = image_tensor.cpu().numpy().transpose((1, 2, 0))\n",
    "    # Desnormalize using ImageNet's mean and std\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    plt.imshow(image)\n",
    "    if title:\n",
    "        plt.title(title, fontsize=8)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Evaluación: iterate over the test_loader and display images with labels and predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, image_names, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        # Compute model outputs\n",
    "        out_nature, out_materiality, out_biological, out_landscape = model(images)\n",
    "        \n",
    "        # Get predictions by taking the argmax for each task\n",
    "        preds_nature = out_nature.argmax(dim=1)\n",
    "        preds_materiality = out_materiality.argmax(dim=1)\n",
    "        preds_biological = out_biological.argmax(dim=1)\n",
    "        preds_landscape = out_landscape.argmax(dim=1)\n",
    "        \n",
    "        # Since the DataLoader's default collate returns a dictionary of lists for labels,\n",
    "        # we assume labels['task'] is a list of integers. If not, adjust accordingly.\n",
    "        # Iterate over each image in the batch\n",
    "        for i in range(len(images)):\n",
    "            # Ground truth labels for this sample\n",
    "            gt_nature = labels['nature_visual'][i]\n",
    "            gt_materiality = labels['nep_materiality_visual'][i]\n",
    "            gt_biological = labels['nep_biological_visual'][i]\n",
    "            gt_landscape = labels['landscape-type_visual'][i]\n",
    "            \n",
    "            # Predictions for this sample (convert tensor to int)\n",
    "            pred_nature = preds_nature[i].item()\n",
    "            pred_materiality = preds_materiality[i].item()\n",
    "            pred_biological = preds_biological[i].item()\n",
    "            pred_landscape = preds_landscape[i].item()\n",
    "            \n",
    "            # Create a title string with image name, ground truth and predictions\n",
    "            title = (f\"Name: {image_names[i]}\\n\"\n",
    "                     f\"GT - Nature: {gt_nature}, Materiality: {gt_materiality}, \"\n",
    "                     f\"Biological: {gt_biological}, Landscape: {gt_landscape}\\n\"\n",
    "                     f\"Pred - Nature: {pred_nature}, Materiality: {pred_materiality}, \"\n",
    "                     f\"Biological: {pred_biological}, Landscape: {pred_landscape}\")\n",
    "            \n",
    "            # Display the image with the title\n",
    "            imshow_tensor(images[i], title=title)\n",
    "        # Break after one batch (or remove the break to process all batches)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
